{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c7ea0e-e71b-40b4-a8a0-5e1eed889b4d",
   "metadata": {},
   "source": [
    "## Training a small neural language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1386c91-72d1-4e33-982b-79d2dff08443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import tqdm\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "EMBEDDING_WIDTH = 100\n",
    "MINIBATCH_SIZE = 5000\n",
    "\n",
    "# Note that there are three special tokens we need to take into account here.\n",
    "# They are:\n",
    "# - <START> -- the special token denoting the beginning of a sentence.\n",
    "#   - It happens twice to start every sentence.\n",
    "#   - Its word ID number is 0.\n",
    "#   - Its embedding is defined to be [1] * EMBEDDING_WIDTH\n",
    "# - </s> -- the end of every sentence!\n",
    "#   - It's how we know to stop generating, and it will be the last entry in a sequence.\n",
    "#   - Its word ID number is 1.\n",
    "#   - This actually does not have an embedding since it will never be in the context.\n",
    "# - <UNK>\n",
    "#   - This is our representation for a token we do not have in our vocabulary! This will happen sometimes,\n",
    "#     since our vocabulary is just words that occurred 5 or more times in the training data.\n",
    "#   - Unknown words get the word ID number 2.\n",
    "#   - Our embedding for <UNK> is defined to be np.zeros(EMBEDDING_WIDTH)\n",
    "\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    \"\"\"Returns a dictionary mapping from words to their embeddings, loading\n",
    "    them from a word2vec vec.txt file.\"\"\"\n",
    "    words_to_embeddings = {}\n",
    "    \n",
    "    words_to_embeddings[\"<START>\"] = np.array([1.0] * EMBEDDING_WIDTH)\n",
    "    words_to_embeddings[\"<UNK>\"] = np.zeros((EMBEDDING_WIDTH,))\n",
    "    \n",
    "    with open(filename) as infile:\n",
    "        # skip the first line\n",
    "        firstline = next(infile)\n",
    "        for line in infile:\n",
    "            tokens = line.strip().split()\n",
    "            word = tokens[0]\n",
    "            embedding = tokens[1:]\n",
    "            embedding = [float(token) for token in embedding]\n",
    "            embedding = np.array(embedding)\n",
    "            words_to_embeddings[word] = embedding\n",
    "    return words_to_embeddings\n",
    "\n",
    "\n",
    "def load_vocabulary(filename):\n",
    "    \"\"\"Returns two dictionaries -- the first maps from words to their IDs and\n",
    "    the second maps from IDs to words. These are also loaded from a word2vec\n",
    "    vec.txt file, for simplicity.\"\"\"\n",
    "\n",
    "    vocab_lookup = {}\n",
    "    vocab_lookup[\"<START>\"] = 0\n",
    "    vocab_lookup[\"</s>\"] = 1\n",
    "    vocab_lookup[\"<UNK>\"] = 2\n",
    "    \n",
    "    index_to_word = {}\n",
    "    index_to_word[0] = \"<START>\"\n",
    "    index_to_word[1] = \"</s>\"\n",
    "    index_to_word[2] = \"<UNK>\"\n",
    "    \n",
    "    with open(filename) as infile:\n",
    "        # skip the first line\n",
    "        firstline = next(infile)\n",
    "        for position, line in enumerate(infile):\n",
    "            tokens = line.strip().split()\n",
    "            word = tokens[0]\n",
    "\n",
    "            # 0th word in the file gets ID 3 and so forth...\n",
    "            word_id = 3 + position\n",
    "            vocab_lookup[word] = word_id\n",
    "            index_to_word[word_id] = word\n",
    "    return vocab_lookup, index_to_word\n",
    "\n",
    "def get_embedding(embeddings, word):\n",
    "    if word in embeddings:\n",
    "        return embeddings[word]\n",
    "    else:\n",
    "        return embeddings[\"<UNK>\"]\n",
    "\n",
    "\n",
    "def get_word_id(word_to_id, word):\n",
    "    if word in word_to_id:\n",
    "        return word_to_id[word]\n",
    "    else:\n",
    "        return word_to_id[\"<UNK>\"]\n",
    "\n",
    "\n",
    "def X_y_for_sentence(sentence, embeddings, word_to_id):\n",
    "    \"\"\"Prepare a single sentence for training.\n",
    "\n",
    "    Returns a list of concatenated embeddings (so 200-wide numpy vectors) and\n",
    "    a corresponding list of word labels.\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    prevprev = \"<START>\"\n",
    "    prev = \"<START>\"\n",
    "    for token in sentence + [\"</s>\"]:\n",
    "        # look up the embeddings\n",
    "        prevprev_embedding = get_embedding(embeddings, prevprev)\n",
    "        prev_embedding = get_embedding(embeddings, prev)\n",
    "\n",
    "        both_embeddings = np.concat([prevprev_embedding, prev_embedding])\n",
    "        X_list.append(both_embeddings)\n",
    "\n",
    "        target = get_word_id(word_to_id, token)\n",
    "        y_list.append(target)\n",
    "        prevprev = prev\n",
    "        prev = token\n",
    "    return X_list, y_list\n",
    "\n",
    "\n",
    "def generate_minibatches(filename, embeddings, vocabulary):\n",
    "    \"\"\"Generator that yields little batches, suitable for training with partial_fit\"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            X_add, y_add = X_y_for_sentence(tokens, embeddings, vocabulary)\n",
    "            X_list.extend(X_add)\n",
    "            y_list.extend(y_add)\n",
    "\n",
    "            if len(X_list) >= MINIBATCH_SIZE:\n",
    "                assert len(X_list) == len(y_list)\n",
    "                X = np.array(X_list)\n",
    "                y = np.array(y_list)\n",
    "                X_list = []\n",
    "                y_list = []\n",
    "                yield (X, y)\n",
    "\n",
    "    assert len(X_list) == len(y_list)\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    yield (X, y)\n",
    "\n",
    "\n",
    "def batch_for_file(filename, embeddings, vocabulary):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            X_add, y_add = X_y_for_sentence(tokens, embeddings, vocabulary)\n",
    "            X_list.extend(X_add)\n",
    "            y_list.extend(y_add)\n",
    "    assert len(X_list) == len(y_list)\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea575b11-5fc3-4d4b-b868-820484a6d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings(\"vec.txt\")\n",
    "vocabulary, index_to_word = load_vocabulary(\"vec.txt\")\n",
    "\n",
    "\n",
    "theclasses = np.unique(np.array([cl for cl in vocabulary.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed26ab-6898-4c42-9c2a-79792fd29d89",
   "metadata": {},
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66d2b833-5644-48f0-9b51-cd71117e8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: You!! Mess with some different Neural network architectures here!!\n",
    "clf = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(200,100), activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea92aed1-abc0-48f1-a551-9f609b9120b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training over minibatches...\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [07:28, 10.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 5.660356319437758\n",
      "validation accuracy 0.15412186379928317\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [07:28, 10.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 5.353364601530019\n",
      "validation accuracy 0.16641065028161803\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:18, 11.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 5.085625978688329\n",
      "validation accuracy 0.17357910906298002\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:19, 11.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.8329657291439565\n",
      "validation accuracy 0.18023553507424475\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:18, 11.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.596961705834991\n",
      "validation accuracy 0.1930363543266769\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:28, 11.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.40867714061539\n",
      "validation accuracy 0.2078853046594982\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:28, 11.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.288401606572446\n",
      "validation accuracy 0.21761392729134665\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:27, 11.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.202507790790791\n",
      "validation accuracy 0.22939068100358423\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:27, 11.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.141266011680082\n",
      "validation accuracy 0.23195084485407066\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:13, 11.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.087337213789286\n",
      "validation accuracy 0.23758320532514082\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:11, 11.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.045708018663329\n",
      "validation accuracy 0.23707117255504354\n",
      "epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:12, 11.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.004087149605253\n",
      "validation accuracy 0.24065540194572452\n",
      "epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:12, 11.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.9740121202868854\n",
      "validation accuracy 0.23809523809523808\n",
      "epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:10, 11.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.9573504756847036\n",
      "validation accuracy 0.24475166410650281\n",
      "epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:11, 11.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.9182753337302687\n",
      "validation accuracy 0.23809523809523808\n",
      "epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:11, 11.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.882516188493801\n",
      "validation accuracy 0.24679979518689196\n",
      "epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:14, 11.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.868019404605622\n",
      "validation accuracy 0.2488479262672811\n",
      "epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:12, 11.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.8520561520393346\n",
      "validation accuracy 0.24372759856630824\n",
      "epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:12, 11.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.845142147950689\n",
      "validation accuracy 0.24270353302611367\n",
      "epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [08:12, 11.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.8238191571466142\n",
      "validation accuracy 0.24372759856630824\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You might want to run for different numbers of epochs. Also it's safe to run this cell several times if you\n",
    "# want to keep training your model incrementally.\n",
    "NUM_EPOCHS = 20\n",
    "X_validation, y_validation = batch_for_file(\n",
    "    \"sentences_validation\", embeddings, vocabulary\n",
    ")\n",
    "\n",
    "print(\"training over minibatches...\")\n",
    "accuracies = []\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"epoch\", epoch)\n",
    "    for batchnum, (X_batch, y_batch) in tqdm.tqdm(\n",
    "        enumerate(generate_minibatches(\"sentences_train\", embeddings, vocabulary))\n",
    "    ):\n",
    "        clf.partial_fit(X_batch, y_batch, classes=theclasses)\n",
    "\n",
    "    y_pred = clf.predict_proba(X_validation)\n",
    "    validation_loss = sklearn.metrics.log_loss(y_validation, y_pred, labels=theclasses)\n",
    "    validation_acc = clf.score(X_validation, y_validation)\n",
    "    accuracies.append(validation_acc)\n",
    "    losses.append(validation_loss)\n",
    "    print(\"validation loss\", validation_loss)\n",
    "    print(\"validation accuracy\", validation_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f387c93-96ec-4659-a481-e98b6b5fb8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss\n",
      "6.627492703994024\n",
      "test accuracy\n",
      "0.16830096171978126\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = batch_for_file(\"sentences_test\", embeddings, vocabulary)\n",
    "\n",
    "print(\"test loss\")\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "test_loss = sklearn.metrics.log_loss(y_test, y_pred, labels=theclasses)\n",
    "print(test_loss)\n",
    "print(\"test accuracy\")\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c77c82-6015-4bc9-b20f-bfa779540e1d",
   "metadata": {},
   "source": [
    "## next word prediction\n",
    "\n",
    "Here is some code to demonstrate how we might get the next possible words by calling the classifier with the embeddings for the two previous words. If the classifier has been trained sensibly, we will tend to favor nouns and adjectives to occur after \"of the\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953e84a0-580e-49ad-80fa-25c7ce197e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 77 &\n",
      "word 4 ,\n",
      "word 16 ;\n",
      "word 14 for\n",
      "word 40 all\n",
      "word 18 said\n",
      "word 10 in\n",
      "word 31 \"\n",
      "word 9 to\n",
      "word 1 </s>\n",
      "word 55 dlrs\n",
      "word 63 its\n",
      "word 74 unto\n",
      "word 36 you\n",
      "word 6 .\n",
      "word 8 of\n",
      "word 71 no\n",
      "word 85 more\n",
      "word 69 2\n",
      "word 33 from\n"
     ]
    }
   ],
   "source": [
    "both_embeddings = np.concat([embeddings[\"<START>\"], embeddings[\"<START>\"]])\n",
    "res = clf.predict_proba(np.array([both_embeddings]))\n",
    "\n",
    "most_prob_indices = np.argsort(-res)\n",
    "for i in range(20):\n",
    "    index = most_prob_indices[0][i]\n",
    "    print(\"word\", index, index_to_word[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1547247c-95c1-41e6-8c02-cad2db649499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_probability_for_sequence(\n",
    "    clf, embeddings, vocabulary, index_to_word, sequence\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the negative log probability (bits of surprise) for a sequence.\n",
    "    \n",
    "    Args:\n",
    "        clf: trained classifier\n",
    "        embeddings: dictionary mapping words to embeddings\n",
    "        vocabulary: dictionary mapping words to IDs\n",
    "        index_to_word: dictionary mapping IDs to words\n",
    "        sequence: list of word strings (not including </s>)\n",
    "    \n",
    "    Returns:\n",
    "        negative log probability in bits (base 2)\n",
    "    \"\"\"\n",
    "    total_neg_log_prob = 0.0\n",
    "    \n",
    "    # Start with the context being [START, START]\n",
    "    prevprev = \"<START>\"\n",
    "    prev = \"<START>\"\n",
    "    \n",
    "    # Process each word in the sequence, plus the final </s> token\n",
    "    for token in sequence + [\"</s>\"]:\n",
    "        # Get embeddings for the context (previous two words)\n",
    "        prevprev_embedding = get_embedding(embeddings, prevprev)\n",
    "        prev_embedding = get_embedding(embeddings, prev)\n",
    "        \n",
    "        # Concatenate embeddings for the context\n",
    "        both_embeddings = np.concat([prevprev_embedding, prev_embedding])\n",
    "        \n",
    "        # Get probability distribution over next words\n",
    "        probs = clf.predict_proba(np.array([both_embeddings]))[0]\n",
    "        \n",
    "        # Get the ID of the actual next token\n",
    "        target_id = get_word_id(vocabulary, token)\n",
    "        \n",
    "        # Get the probability assigned to the actual next token\n",
    "        target_prob = probs[target_id]\n",
    "        \n",
    "        # Add negative log (base 2) to running total\n",
    "        # Using log2 to get bits of surprise\n",
    "        neg_log_prob = -np.log2(target_prob)\n",
    "        total_neg_log_prob += neg_log_prob\n",
    "        \n",
    "        # Update context for next iteration\n",
    "        prevprev = prev\n",
    "        prev = token\n",
    "    \n",
    "    return total_neg_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0669116e-d14a-4eeb-84c5-6ee0b2126b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sequences_from_file(filename, shuffled=False):\n",
    "    output = []\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            tokens = line.split()\n",
    "            if shuffled:\n",
    "                random.shuffle(tokens)\n",
    "            output.append(tokens)\n",
    "    return output\n",
    "\n",
    "\n",
    "sequences = sequences_from_file(\"sentences_test\")\n",
    "sequences_shuffled = sequences_from_file(\"sentences_test\", shuffled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceb8dd1f-eec8-48a7-80e7-a64383120bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing negative log-probs for original test sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 22.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing negative log-probs for shuffled test sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:47<00:00, 20.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average negative log-prob (bits of surprise) for ORIGINAL sequences: 328.96\n",
      "Average negative log-prob (bits of surprise) for SHUFFLED sequences: 329.90\n",
      "\n",
      "Difference: 0.94 bits\n",
      "The model is LESS surprised by the original sequences!\n",
      "\n",
      "--- Sample Comparisons ---\n",
      "\n",
      "Sentence 1:\n",
      "  Original: -- farm machinery dealer bob houtz tilts back in a battered chair and tells of a sharp pickup in sales : `` we've sold four corn pickers since labor day and have good prospects for 10 more .\n",
      "  Original surprise: 682.76 bits\n",
      "  Shuffled: in dealer sales a good pickup four houtz and chair tells . farm sharp battered back bob pickers and labor we've a since 10 corn : more prospects of tilts `` machinery for have day sold in --\n",
      "  Shuffled surprise: 607.87 bits\n",
      "\n",
      "Sentence 2:\n",
      "  Original: there are two types of such intersections , depending essentially on whether the curves cross at the point of intersection .\n",
      "  Original surprise: 381.95 bits\n",
      "  Shuffled: the at on , there two essentially the of whether intersections curves point of types intersection depending . such are cross\n",
      "  Shuffled surprise: 413.02 bits\n",
      "\n",
      "Sentence 3:\n",
      "  Original: but due to the many applicants on file , would he co-operate and write a personal letter giving them his son's motivation , interests and his qualifications for leadership ? ?\n",
      "  Original surprise: 555.46 bits\n",
      "  Shuffled: ? personal and his , leadership giving a motivation for write but applicants qualifications them the son's many ? , and letter on co-operate file due to interests his would he\n",
      "  Shuffled surprise: 513.90 bits\n",
      "\n",
      "Sentence 4:\n",
      "  Original: he shares with mr. morse a parody of the college anthems he once sang while his second song is whisked away from him by virginia martin , a girl with a remarkably expressive yip in her voice .\n",
      "  Original surprise: 667.42 bits\n",
      "  Shuffled: virginia sang . once away expressive second song with his he parody the girl a shares from with morse anthems while he remarkably a , martin her voice college by of mr. whisked yip a him in is\n",
      "  Shuffled surprise: 637.99 bits\n",
      "\n",
      "Sentence 5:\n",
      "  Original: religion provides the most attractive rewards , either in this world or the next , for those who not merely abide by its norms , but who engage in good works .\n",
      "  Original surprise: 464.97 bits\n",
      "  Shuffled: , its , but attractive engage either world who rewards norms good most provides in the works not or . in by the merely for those , next this abide religion who\n",
      "  Shuffled surprise: 519.89 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Here you'll run negative_log_probability_for_sequence on the elements of the lists sequences and sequences_shuffled.\n",
    "\n",
    "# Calculate negative log probabilities for original sequences\n",
    "print(\"Computing negative log-probs for original test sequences...\")\n",
    "neg_log_probs_original = []\n",
    "for seq in tqdm.tqdm(sequences):\n",
    "    nlp = negative_log_probability_for_sequence(clf, embeddings, vocabulary, index_to_word, seq)\n",
    "    neg_log_probs_original.append(nlp)\n",
    "\n",
    "# Calculate negative log probabilities for shuffled sequences\n",
    "print(\"\\nComputing negative log-probs for shuffled test sequences...\")\n",
    "neg_log_probs_shuffled = []\n",
    "for seq in tqdm.tqdm(sequences_shuffled):\n",
    "    nlp = negative_log_probability_for_sequence(clf, embeddings, vocabulary, index_to_word, seq)\n",
    "    neg_log_probs_shuffled.append(nlp)\n",
    "\n",
    "# Calculate average negative log probabilities\n",
    "avg_original = np.mean(neg_log_probs_original)\n",
    "avg_shuffled = np.mean(neg_log_probs_shuffled)\n",
    "\n",
    "print(f\"\\nAverage negative log-prob (bits of surprise) for ORIGINAL sequences: {avg_original:.2f}\")\n",
    "print(f\"Average negative log-prob (bits of surprise) for SHUFFLED sequences: {avg_shuffled:.2f}\")\n",
    "print(f\"\\nDifference: {avg_shuffled - avg_original:.2f} bits\")\n",
    "print(f\"The model is {'LESS' if avg_original < avg_shuffled else 'MORE'} surprised by the original sequences!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n--- Sample Comparisons ---\")\n",
    "for i in range(min(5, len(sequences))):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(f\"  Original: {' '.join(sequences[i])}\")\n",
    "    print(f\"  Original surprise: {neg_log_probs_original[i]:.2f} bits\")\n",
    "    print(f\"  Shuffled: {' '.join(sequences_shuffled[i])}\")\n",
    "    print(f\"  Shuffled surprise: {neg_log_probs_shuffled[i]:.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4c50e-b108-4b49-a9d8-127bc8e7fe73",
   "metadata": {},
   "source": [
    "## The trained model prefers the non-shuffled sequences to the shuffled ones\n",
    "\n",
    "* The negative log-probs of the sequences in the test set should be lower for the naturally-occuring orders than the shuffled ones.\n",
    "* Note that in general, the \"bits of surprise\" for a longer sequence will be greater than for a shorter one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bad8ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequences preferred: 486/1000\n",
      "Shuffled sequences preferred: 514/1000\n",
      "\n",
      "Average negative log-probability:\n",
      "  Original sequences: 328.96\n",
      "  Shuffled sequences: 329.90\n",
      "  Difference: 0.94\n",
      "\n",
      "Model shows preference for natural word order\n"
     ]
    }
   ],
   "source": [
    "count_original_better = 0\n",
    "count_shuffled_better = 0\n",
    "\n",
    "for i, (orig, shuf) in enumerate(zip(neg_log_probs_original, neg_log_probs_shuffled)):\n",
    "    if orig < shuf:  # Lower NLP = higher probability = better\n",
    "        count_original_better += 1\n",
    "    else:\n",
    "        count_shuffled_better += 1\n",
    "\n",
    "avg_orig = np.mean(neg_log_probs_original)\n",
    "avg_shuf = np.mean(neg_log_probs_shuffled)\n",
    "\n",
    "print(f\"Original sequences preferred: {count_original_better}/{len(neg_log_probs_original)}\")\n",
    "print(f\"Shuffled sequences preferred: {count_shuffled_better}/{len(neg_log_probs_shuffled)}\")\n",
    "print(f\"\\nAverage negative log-probability:\")\n",
    "print(f\"  Original sequences: {avg_orig:.2f}\")\n",
    "print(f\"  Shuffled sequences: {avg_shuf:.2f}\")\n",
    "print(f\"  Difference: {avg_shuf - avg_orig:.2f}\")\n",
    "print(f\"\\nModel shows {'preference' if avg_orig < avg_shuf else 'NO preference'} for natural word order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9a8d4-87ac-48dd-b8a1-34a1d2a776d5",
   "metadata": {},
   "source": [
    "# Sample sentences from your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd296fc-f1c5-4346-8328-17ed40a0c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(clf, embeddings, vocabulary, index_to_word):\n",
    "    \"\"\"\n",
    "    Sample a sentence from the trained language model.\n",
    "    \n",
    "    Args:\n",
    "        clf: trained classifier\n",
    "        embeddings: dictionary mapping words to embeddings\n",
    "        vocabulary: dictionary mapping words to IDs\n",
    "        index_to_word: dictionary mapping IDs to words\n",
    "    \n",
    "    Returns:\n",
    "        list of strings representing the generated sentence\n",
    "    \"\"\"\n",
    "    # Start with context [START, START]\n",
    "    prevprev = \"<START>\"\n",
    "    prev = \"<START>\"\n",
    "    \n",
    "    generated_sequence = []\n",
    "    \n",
    "    # Keep generating until we get </s> token or hit a maximum length\n",
    "    max_length = 100  # Safety limit to prevent infinite loops\n",
    "    \n",
    "    while len(generated_sequence) < max_length:\n",
    "        # Get embeddings for the context\n",
    "        prevprev_embedding = get_embedding(embeddings, prevprev)\n",
    "        prev_embedding = get_embedding(embeddings, prev)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        both_embeddings = np.concat([prevprev_embedding, prev_embedding])\n",
    "        \n",
    "        # Get probability distribution over next words\n",
    "        probs = clf.predict_proba(np.array([both_embeddings]))[0]\n",
    "        \n",
    "        # Sample from the probability distribution\n",
    "        # np.random.choice takes probabilities and returns an index\n",
    "        sampled_id = np.random.choice(len(probs), p=probs)\n",
    "        \n",
    "        # Convert ID to word\n",
    "        sampled_word = index_to_word[sampled_id]\n",
    "        \n",
    "        # If we hit the end token, stop\n",
    "        if sampled_word == \"</s>\":\n",
    "            break\n",
    "        \n",
    "        # Add to generated sequence\n",
    "        generated_sequence.append(sampled_word)\n",
    "        \n",
    "        # Update context\n",
    "        prevprev = prev\n",
    "        prev = sampled_word\n",
    "    \n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c52a827c-9682-4ad3-9c5c-53b61fd9b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample sentences from the trained language model:\n",
      "\n",
      "1. 2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. employment . <UNK> of archer their step sheet to , deferred the original estimate and expected the dripping dlrs and alongside , . <UNK>\n",
      "\n",
      "3. for various accustomed the resistors jack to all 20 doth broken which seems there mln $2 war\n",
      "\n",
      "4. , and , have had ; it and 182 porter if said : ; it from my 19 it if said and left fall hospital\n",
      "\n",
      "5. . <UNK> into , of compared the nickname o <UNK> of come very rise to juncture ' labor of aperture have 3 conquest and moscow of college married case from below very receive must his having around smith\n",
      "\n",
      "6. ; <UNK> party vs\n",
      "\n",
      "7. to miracles year like value and , \" catch 2\n",
      "\n",
      "8. ; <UNK> , , to march stock river was either and art she co stairway mln it\n",
      "\n",
      "9. all announce from with men the pact if tonnes u <UNK> thereafter to his synonymous friend . <UNK> and cts and being vain the cities you his shr election that ? she 17\n",
      "\n",
      "10. triumphs vs <UNK>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Generated Sentences from the Model\n",
    "\n",
    "# Generate and display 10 sample sentences\n",
    "print(\"Generating sample sentences from the trained language model:\\n\")\n",
    "for i in range(10):\n",
    "    sampled_sentence = sample_from_model(clf, embeddings, vocabulary, index_to_word)\n",
    "    print(f\"{i+1}. {' '.join(sampled_sentence)}\")\n",
    "    print()\n",
    "\n",
    "# Note: These sentences may not always be grammatically perfect or semantically coherent,\n",
    "# but they should demonstrate that the model has learned some patterns about word sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
